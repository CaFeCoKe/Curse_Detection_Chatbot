{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YOsvsWbB3cbE6s-qNOCLxcoUN46mcZ5y","authorship_tag":"ABX9TyNUi/x4HH4rMJ1DFbRlno3v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 사전 준비"],"metadata":{"id":"ArB0Rdtrxz2C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"59nVSMa4xmCw"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"markdown","source":["**경고 메시지 끄기**"],"metadata":{"id":"h7gGvQMgx3__"}},{"cell_type":"code","source":["import warnings\n","\n","# 경고메세지 끄기\n","warnings.filterwarnings(action='ignore')"],"metadata":{"id":"8rHck63jx2BM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**사전학습모델과 토크나이저(KcELECTRA, KoGPT2) 불러오기**"],"metadata":{"id":"xUDuKx70yWOP"}},{"cell_type":"code","source":["from transformers import ElectraTokenizer, ElectraForSequenceClassification, GPT2LMHeadModel, PreTrainedTokenizerFast\n","\n","ELECTRA_tokenizer = ElectraTokenizer.from_pretrained(\"beomi/KcELECTRA-base\", padding_side='left')\n","ELECTRA_model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\")   # 비속어 감지의 사전학습 모델로는 KcELECTRA 사용\n","\n","GPT2_tokenizer = PreTrainedTokenizerFast.from_pretrained(\n","    \"skt/kogpt2-base-v2\",\n","    bos_token='</s>',       # 문장 시작토큰\n","    eos_token='</s>',       # 문장 마지막토큰\n","    unk_token='<unk>',      # 어휘에 없는 토큰\n","    pad_token='<pad>',      # 크기 맞추기 토큰\n","    mask_token='<mask>',     # 마스킹 토큰\n",")\n","GPT2_model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')   # 챗봇의 사전학습 모델로는 KoGPT2 사용"],"metadata":{"id":"SZueJLPryAaK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 불러오기"],"metadata":{"id":"hCjhz76s0DUL"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import urllib.request"],"metadata":{"id":"ycXm_FCI1FXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["curse_data = pd.read_table(\"/content/drive/Othercomputers/내 컴퓨터/Curse_Detection_Chatbot/Curse-detection-data/dataset.txt\", \n","                           names=[\"text\", \"label\"], sep=\"|\", header=None)\n","curse_data"],"metadata":{"id":"uk2dCRrC0C8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(\"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\", filename=\"ChatBotData.csv\")\n","chat_data = pd.read_csv('ChatBotData.csv')\n","\n","chat_data"],"metadata":{"id":"2WGN5h_B1Jr-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 텍스트 전처리\n","\n","**전처리 함수 정의**"],"metadata":{"id":"NIWBMkxAIvKL"}},{"cell_type":"code","source":["!pip install soynlp\n","!pip install emoji==1.7.0"],"metadata":{"id":"lVt4AwD8ItP1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import emoji\n","from soynlp.normalizer import repeat_normalize"],"metadata":{"id":"Z_l5dNBQIwSS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n","pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n","url_pattern = re.compile(\n","    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n","special_symbol = re.compile(\n","    r'([.,?!/@$%~％·∼()\\x21-\\x2F\\x3A-\\x40\\x5B-\\x60\\x7B-\\x7E])\\1{1,}')"],"metadata":{"id":"4GGxfQXAIxaF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean(x):\n","    x = pattern.sub(' ', x)                     # 일반적으로 사용하는 특수문자, 영어, 한글, emoji제외 공백으로 치환\n","    x = url_pattern.sub('', x)                  # URL 제거\n","    x = special_symbol.sub('\\\\1'*1, x)          # 반복되는 특수문자의 축약 횟수 1개로 줄임\n","    x = x.strip()                               # 문자의 시작과 끝에서 공백제거\n","    x = repeat_normalize(x, num_repeats=2)      # 반목되는 문자의 축약 횟수 2개로 줄임\n","    return x"],"metadata":{"id":"WoylaJWPIybG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**전처리 및 데이터 분할**\n","\n","텍스트 전처리는 비속어 감지 데이터에만 적용한다. (챗봇 데이터와 달리 댓글의 성격을 띄기 떄문에 무의미한 반복의 텍스트가 존재하기 때문)"],"metadata":{"id":"kXn-L2RQxYve"}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"_-DJ0REM2P_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 비속어 감지 데이터\n","# train : validation = 3 : 1 (5824 = 2^6 * 7 * 13)\n","\n","train_curse_text = [clean(curse_data['text'][idx]) for idx in range(0, int((curse_data.shape[0]/4)*3))]\n","val_curse_text = [clean(curse_data['text'][idx]) for idx in range(int((curse_data.shape[0]/4)*3), int((curse_data.shape[0]/4)*4))]\n","\n","train_curse_label = [curse_data['label'][idx] for idx in range(0, int((curse_data.shape[0]/4)*3))]\n","val_curse_label = [curse_data['label'][idx] for idx in range(int((curse_data.shape[0]/4)*3), int((curse_data.shape[0]/4)*4))]"],"metadata":{"id":"mOZLmf0Jx3d-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 챗봇 데이터(Q, A)\n","# train : validation = 5 : 2 (11823 = 3 * 7 * 563)\n","\n","train_chat_data = pd.DataFrame(chat_data.iloc[idx] for idx in range (0, int((chat_data.shape[0]/7)*5)))\n","val_chat_data = pd.DataFrame(chat_data.iloc[idx] for idx in range (int((chat_data.shape[0]/7)*5), int((chat_data.shape[0]/7)*7)))"],"metadata":{"id":"wGLsubHlyj4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 구축, 토크나이징"],"metadata":{"id":"_tEN1IPJ9VcA"}},{"cell_type":"markdown","source":["**KoGPT2 스페셜토큰 확인**"],"metadata":{"id":"ZJymZeDdoq6O"}},{"cell_type":"code","source":["for i in range (10):\n","    print(\"index : \",i,\" =  tokens : \",GPT2_tokenizer.decode(i))"],"metadata":{"id":"rRA_kK5WovhG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**비속어 감지 데이터**"],"metadata":{"id":"XmbF3neEKMeC"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader"],"metadata":{"id":"KrkN1mPG9nlQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토크나이징\n","\n","train_input_token = ELECTRA_tokenizer(train_curse_text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")\n","val_input_token = ELECTRA_tokenizer(val_curse_text, truncation=True, padding=True, max_length=256, return_tensors=\"pt\")"],"metadata":{"id":"c-rSQGyX9zwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 비속어 감지 데이터셋\n","\n","class CurseDataset(Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.inputs.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"],"metadata":{"id":"hojNXzZA9xXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_curse_dataset = CurseDataset(train_input_token, train_curse_label)\n","val_curse_dataset = CurseDataset(val_input_token, val_curse_label)"],"metadata":{"id":"0b5LvCewCC_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터로더\n","\n","train_curse_loader = DataLoader(train_curse_dataset, shuffle=True, batch_size=8)\n","val_curse_loader = DataLoader(val_curse_dataset, shuffle=True, batch_size=8)"],"metadata":{"id":"MNXTzyTIDfm3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**챗봇 데이터**"],"metadata":{"id":"-RnFeGCWFq8h"}},{"cell_type":"code","source":["# 챗봇 데이터셋 (토크나이징 포함)\n","\n","class ChatbotDataset(Dataset):\n","    def __init__(self, chats, max_len=64):  # 데이터셋의 전처리를 해주는 부분\n","        self._data = chats\n","        self.max_len = max_len\n","        self.q_token = \"<usr>\"           # 새로 추가된 special token을 사용\n","        self.a_token = \"<sys>\"\n","        self.bos = GPT2_tokenizer.bos_token\n","        self.eos = GPT2_tokenizer.eos_token\n","        self.mask = GPT2_tokenizer.mask_token\n","        self.tokenizer = GPT2_tokenizer\n","\n","    def __len__(self):  # chatbotdata 의 길이를 리턴\n","        return len(self._data)\n","\n","    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n","        index = self._data.iloc[idx]\n","\n","        q = index[\"Q\"]  # 질문\n","        q_toked = self.tokenizer.tokenize(self.bos + self.q_token + q)      #   질문\n","        q_len = len(q_toked)\n","\n","        a = index[\"A\"]  # 답변\n","        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)      #  답 \n","        a_len = len(a_toked)\n","\n","        # 질문의 길이가 최대길이보다 클때\n","        if q_len > self.max_len: \n","            q_toked = q_toked[-(int(self.max_len / 2)):]   # 질문길이를 최대길이의 반으로 \n","            q_len = len(q_toked)\n","\n","        # 질문 + 답변 길이가 최대길이보다 클때\n","        if q_len + a_len > self.max_len:\n","            a_len = self.max_len - q_len        # 답변의 길이 = 최대길이 - 질문길이\n","\n","            if a_len <= 0:       # 질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n","                q_toked = q_toked[-(int(self.max_len / 2)) :]   # 질문길이를 최대길이의 반으로 \n","                q_len = len(q_toked)\n","                a_len = self.max_len - q_len              # 답변의 길이를 최대길이 - 질문길이\n","                \n","            a_toked = a_toked[:a_len]\n","            a_len = len(a_toked)\n","\n","        # 질문 + 답변 토큰을 index로 변환   \n","        token = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n","        # 최대길이만큼 padding\n","        while len(token) < self.max_len:\n","            token += [self.tokenizer.pad_token_id]\n","\n","        # attention(어텐션마스크) = 질문+답변 길이 1 + 나머지(패딩) 0\n","        attention = [1]*(q_len+a_len) + [0]*(self.max_len - q_len - a_len)\n","\n","        # token_type_ids(세그먼트 정보) = 질문 0 + 답변 1 + 나머지 0\n","        token_type = [0]*q_len + [1]*a_len + [0]*(self.max_len - q_len - a_len)\n","\n","        label = q_toked[0:2] + [self.mask,]*(q_len-2) + a_toked[0:]\n","        # label을 index로 변환\n","        label = self.tokenizer.convert_tokens_to_ids(label)\n","        # 최대길이만큼 padding\n","        while len(label) < self.max_len:\n","            label += [self.tokenizer.pad_token_id]\n","\n","        \n","        # 질문 + 답변, 어텐션마스크, 세그먼트 정보, 답변\n","        return (token, attention, token_type, label)"],"metadata":{"id":"Ipjn_kaW2aaX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_chat_dataset = ChatbotDataset(train_chat_data, max_len=64)\n","val_chat_dataset = ChatbotDataset(val_chat_data, max_len=64)"],"metadata":{"id":"YS1oPg463RC7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# collate_fn 구성\n","\n","def collate_batch(batch):\n","    token_ids = [item[:][0] for item in batch]\n","    attention_mask = [item[:][1] for item in batch]\n","    token_tpye_ids = [item[:][2] for item in batch]\n","    label_ids = [item[:][3] for item in batch]\n","\n","    return torch.LongTensor(token_ids), torch.LongTensor(attention_mask), torch.LongTensor(token_tpye_ids), torch.LongTensor(label_ids)"],"metadata":{"id":"KuYU9zPZ7Itk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터로더\n","\n","train_chat_loader = DataLoader(train_chat_dataset, shuffle=True, collate_fn = collate_batch, batch_size=16)\n","val_chat_loader = DataLoader(val_chat_dataset, shuffle=True, collate_fn = collate_batch, batch_size=16)"],"metadata":{"id":"CS9o8aYL3lOU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 학습\n","\n","**모델 파라미터 설정**"],"metadata":{"id":"c-NO6KunYUao"}},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","from tqdm.auto import tqdm as tqdm_auto\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"ZSskVemx8amp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU 가속을 사용할 수 있으면 device를 cuda로 설정하고, 아니면 cpu로 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"WU0uCRTiYZTv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KcELECTRA 파라미터 설정\n","ELECTRA_epochs = 5\n","ELECTRA_learning_rate = 5e-5\n","\n","ELECTRA_optimizer = torch.optim.AdamW(ELECTRA_model.parameters(), lr=ELECTRA_learning_rate)\n","ELECTRA_criterion = torch.nn.CrossEntropyLoss()\n","\n","ELECTRA_step = 0\n","ELECTRA_eval_steps = len(train_curse_loader)        # 훈련 배치수"],"metadata":{"id":"qn1U12XJYmeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoGPT2 파라미터 설정\n","GPT2_epoch = 10\n","GPT2_learning_rate = 1e-4\n","\n","GPT2_optimizer = torch.optim.AdamW(GPT2_model.parameters(), lr=GPT2_learning_rate)\n","\n","GPT2_step = 0\n","GPT2_eval_steps = len(train_chat_loader)        # 훈련 배치수"],"metadata":{"id":"GVoyYyHKYgky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ELECTRA_model.to(device)"],"metadata":{"id":"EuFq8XiQZ94W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GPT2_model.to(device)"],"metadata":{"id":"sqSzLcwFaFt5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**KcELECTRA 학습 진행**"],"metadata":{"id":"Yr5weAPS8NU1"}},{"cell_type":"code","source":["for epoch in range(ELECTRA_epochs):\n","    n = 0\n","    train_accuracy = 0\n","    loss = 0\n","    train_loss = 0.0\n","    \n","    ELECTRA_model.train()\n","    for batch in tqdm_auto(train_curse_loader, mininterval=0.01, leave=True):\n","        ELECTRA_optimizer.zero_grad()     # 그래디언트 초기화\n","\n","        # 배치에서 label을 제외한 입력만 추출하여 GPU로 복사\n","        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n","        labels = batch['labels'].to(device)     # 배치에서 라벨을 추출하여 GPU로 복사\n","        outputs = ELECTRA_model(**inputs).logits    # 모형으로 결과 예측\n","\n","        loss = ELECTRA_criterion(outputs, labels)\n","        train_loss += loss\n","        \n","        loss.backward()\n","        ELECTRA_optimizer.step()\n","\n","        # eval_steps 마다 loss를 출력\n","        ELECTRA_step += 1\n","        if ELECTRA_step % ELECTRA_eval_steps == 0:\n","            i = 0\n","            val_accuracy = 0\n","\n","            with torch.no_grad():   # 학습 X (그래디언트 계산 X)\n","                val_loss = 0\n","                ELECTRA_model.eval()        # 평가모드로 전환\n","\n","                for val_batch in tqdm_auto(val_curse_loader, mininterval=0.01, leave=True):\n","\n","                    # 배치에서 label을 제외한 입력만 추출하여 GPU로 복사\n","                    inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'} \n","                    val_labels = batch['labels'].to(device)     # 배치에서 라벨을 추출하여 GPU로 복사\n","                    val_outputs = ELECTRA_model(**inputs).logits     # 모형으로 결과 예측\n","\n","                    loss = ELECTRA_criterion(val_outputs, val_labels)  \n","                    val_loss += loss\n","\n","                    val_accuracy += accuracy_score(val_labels.cpu(), val_outputs.argmax(dim=1).cpu())\n","                    i += 1\n","\n","                avg_val_loss = val_loss / len(val_curse_loader)\n","\n","            val_accuracy /= i\n","            print('Step %d, validation loss: %.4f, accuracy_score: %.2f' % (ELECTRA_step, avg_val_loss, val_accuracy))\n","            \n","        avg_train_loss = train_loss / len(train_curse_loader)\n","        train_accuracy += accuracy_score(labels.cpu(), outputs.argmax(dim=1).cpu())\n","        n += 1\n","\n","    train_accuracy /= n\n","    print('epoch %d, train loss: %.4f, accuracy_score: %.2f \\n' % (epoch, avg_train_loss, train_accuracy))"],"metadata":{"id":"h4dnLooA8VTJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**KoGPT2 학습 진행**"],"metadata":{"id":"YGGks9kE51bT"}},{"cell_type":"code","source":["# GPU 캐시 비우기 (GPU 메모리 확보)\n","torch.cuda.empty_cache()"],"metadata":{"id":"pPJQebVW7VdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(GPT2_epoch):\n","    loss = 0\n","    train_loss = 0.0\n","    \n","    GPT2_model.train()\n","    for batch_idx, samples in enumerate(tqdm_auto(train_chat_loader, mininterval=0.01, leave=True)):\n","        GPT2_optimizer.zero_grad()       # optimizer 초기화(Gradient)\n","\n","        # 모델 입력 텐서 GPU에 올리기\n","        token_ids, attention_mask, token_type_ids, label_ids = samples\n","        token_ids = token_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        token_type_ids = token_type_ids.to(device)\n","        label_ids = label_ids.to(device)\n","\n","        out = GPT2_model(\n","            input_ids=token_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            labels=label_ids,\n","            )\n","        \n","        loss = out.loss\n","        loss.backward()\n","        GPT2_optimizer.step()\n","        \n","        train_loss += loss.item()\n","\n","        # GPU 캐시 비우기 (GPU 메모리 확보)\n","        torch.cuda.empty_cache()\n","        \n","        # eval_steps 마다 loss를 출력\n","        GPT2_step += 1\n","        if GPT2_step % GPT2_eval_steps == 0:\n","            n = 0\n","\n","            with torch.no_grad():   # 학습 X (그래디언트 계산 X)\n","                val_loss = 0\n","                GPT2_model.eval()\n","\n","                for val_batch_idx, val_samples in enumerate(tqdm_auto(val_chat_loader, mininterval=0.01, leave=True)):\n","                    # 모델 입력 텐서 GPU에 올리기\n","                    val_token_ids, val_attention_mask, val_token_type_ids, val_label_ids = val_samples\n","                    val_token_ids = val_token_ids.to(device)\n","                    val_attention_mask = val_attention_mask.to(device)\n","                    val_token_type_ids = val_token_type_ids.to(device)\n","                    val_label_ids = val_label_ids.to(device)\n","\n","                    val_out = GPT2_model(\n","                            input_ids=val_token_ids,\n","                            attention_mask=val_attention_mask,\n","                            token_type_ids=val_token_type_ids,\n","                            labels=val_label_ids,\n","                            )\n","                    loss = out.loss\n","\n","                    val_loss += loss.item()\n","                    torch.cuda.empty_cache()\n","                    \n","                val_loss /= val_batch_idx\n","\n","            print('Step %d, validation loss: %.4f' % (GPT2_step, val_loss))\n","    \n","    # GPT2_scheduler.step()\n","    train_loss /= batch_idx\n","    print('epoch %d, train loss: %.4f \\n' % (epoch, train_loss))"],"metadata":{"id":"I1oWgrpl51Ik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**테스트**"],"metadata":{"id":"VVHe5f0mbd1J"}},{"cell_type":"code","source":["import sys"],"metadata":{"id":"QF3bk3uADcK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_curse = 0\n","print(\"비속어 3회 입력(경고 3번)시 강제종료 됩니다.\\n\")\n","\n","while 1:\n","\n","    q = input(\"user > \").strip()\n","    # quit 입력시 챗봇 종료\n","    if q == \"quit\":\n","        break\n","\n","    clean_text = clean(q)\n","    ELEC_input = ELECTRA_tokenizer.encode(clean_text, return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        output = ELECTRA_model(ELEC_input).logits\n","\n","        if output.argmax(dim=1).cpu() == 1:\n","            num_curse += 1\n","            if num_curse == 3:\n","                sys.exit(\"3회 경고 누적으로 강제종료 됩니다.\")\n","            print(\"비속어가 감지 되었습니다. %d회 경고입니다.\\n\" % (num_curse))\n","\n","        else:\n","            GPT2_input = GPT2_tokenizer.encode(clean_text, return_tensors=\"pt\").to(device)\n","            gen_ids = GPT2_model.generate(\n","                GPT2_input,\n","                do_sample=True,\n","                max_length=30,\n","                top_p=0.5,\n","                top_k=5,\n","                repetition_penalty=1.0,\n","                no_repeat_ngram_size=2,\n","                temperature=0.5,\n","            )\n","    \n","            generated = GPT2_tokenizer.decode(gen_ids[0])\n","            generated = generated[generated.index(\"<sys>\")+5 : generated.index(\"</s>\")]\n","            \n","            print(f'Chatbot > {generated}')"],"metadata":{"id":"HYL1pGg0aHB6"},"execution_count":null,"outputs":[]}]}